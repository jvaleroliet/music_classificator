{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.transforms import Compose\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torchvision import  models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['psytrance', 'house', 'dupstep', 'hardcore_breaks', 'techno']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_subdirectories(directory_path):\n",
    "\n",
    "    subdirectories = []\n",
    "    for item in os.listdir(directory_path):\n",
    "        if os.path.isdir(os.path.join(directory_path, item)):\n",
    "            subdirectories.append(item)\n",
    "\n",
    "    return subdirectories\n",
    "\n",
    "directory_path = \"./songs\"\n",
    "genres = list_subdirectories(directory_path)\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(genres):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'psytrance': 0, 'house': 1, 'dupstep': 2, 'hardcore_breaks': 3, 'techno': 4},\n",
       " {0: 'psytrance', 1: 'house', 2: 'dupstep', 3: 'hardcore_breaks', 4: 'techno'})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id,id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transformation, num_samples,\n",
    "                target_sample_rate, device):\n",
    "        self.root_dir = root_dir\n",
    "        self.genres = os.listdir(root_dir)\n",
    "        self.files = {}\n",
    "        for genre in self.genres:\n",
    "            self.files[genre] = os.listdir(os.path.join(root_dir, genre))\n",
    "\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(files) for files in self.files.values())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        genre = None\n",
    "        file_idx = 0\n",
    "        for g, files in self.files.items():\n",
    "            if idx < len(files):\n",
    "                genre = g\n",
    "                file_idx = idx\n",
    "                break\n",
    "            else:\n",
    "                idx -= len(files)\n",
    "        audio_path = os.path.join(self.root_dir, genre, self.files[genre][file_idx])\n",
    "        genre = label2id[genre]\n",
    "        # Load audio file\n",
    "        signal, sr = torchaudio.load(audio_path, format=\"mp3\")\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        signal = signal.repeat(3, 1, 1)\n",
    "        # signal = signal.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        return signal, genre\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = 16000*3\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64,\n",
    "        normalized = True\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = GenreDataset(\"./songs/\",\n",
    "                            # transformations,\n",
    "                            mel_spectrogram,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 375 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(torch_dataset)} samples in the dataset.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[4.4472e-03, 1.1521e+00, 2.6496e+01,  ..., 1.4050e+01,\n",
       "           1.6485e+01, 6.1929e+01],\n",
       "          [2.3156e-02, 9.2341e+00, 4.8718e+01,  ..., 6.4142e+00,\n",
       "           4.3728e+00, 1.0904e+01],\n",
       "          [2.5428e-01, 5.8940e+00, 4.9396e+00,  ..., 1.7173e+00,\n",
       "           5.6762e+00, 1.7210e+00],\n",
       "          ...,\n",
       "          [6.6990e-07, 3.8549e-11, 2.6087e-10,  ..., 8.3255e-11,\n",
       "           1.1596e-10, 1.9073e-06],\n",
       "          [5.5068e-07, 1.7485e-11, 2.8538e-11,  ..., 1.0314e-10,\n",
       "           2.1542e-10, 2.0002e-06],\n",
       "          [4.9015e-07, 1.4572e-11, 8.7152e-11,  ..., 2.9891e-11,\n",
       "           6.0339e-11, 2.0758e-06]],\n",
       " \n",
       "         [[4.4472e-03, 1.1521e+00, 2.6496e+01,  ..., 1.4050e+01,\n",
       "           1.6485e+01, 6.1929e+01],\n",
       "          [2.3156e-02, 9.2341e+00, 4.8718e+01,  ..., 6.4142e+00,\n",
       "           4.3728e+00, 1.0904e+01],\n",
       "          [2.5428e-01, 5.8940e+00, 4.9396e+00,  ..., 1.7173e+00,\n",
       "           5.6762e+00, 1.7210e+00],\n",
       "          ...,\n",
       "          [6.6990e-07, 3.8549e-11, 2.6087e-10,  ..., 8.3255e-11,\n",
       "           1.1596e-10, 1.9073e-06],\n",
       "          [5.5068e-07, 1.7485e-11, 2.8538e-11,  ..., 1.0314e-10,\n",
       "           2.1542e-10, 2.0002e-06],\n",
       "          [4.9015e-07, 1.4572e-11, 8.7152e-11,  ..., 2.9891e-11,\n",
       "           6.0339e-11, 2.0758e-06]],\n",
       " \n",
       "         [[4.4472e-03, 1.1521e+00, 2.6496e+01,  ..., 1.4050e+01,\n",
       "           1.6485e+01, 6.1929e+01],\n",
       "          [2.3156e-02, 9.2341e+00, 4.8718e+01,  ..., 6.4142e+00,\n",
       "           4.3728e+00, 1.0904e+01],\n",
       "          [2.5428e-01, 5.8940e+00, 4.9396e+00,  ..., 1.7173e+00,\n",
       "           5.6762e+00, 1.7210e+00],\n",
       "          ...,\n",
       "          [6.6990e-07, 3.8549e-11, 2.6087e-10,  ..., 8.3255e-11,\n",
       "           1.1596e-10, 1.9073e-06],\n",
       "          [5.5068e-07, 1.7485e-11, 2.8538e-11,  ..., 1.0314e-10,\n",
       "           2.1542e-10, 2.0002e-06],\n",
       "          [4.9015e-07, 1.4572e-11, 8.7152e-11,  ..., 2.9891e-11,\n",
       "           6.0339e-11, 2.0758e-06]]], device='mps:0'),\n",
       " 0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 32])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset[0][0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(genres)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/letstrythis/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/letstrythis/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size of the validation set as 10% of the training data.\n",
    "val_size = int(len(torch_dataset)*0.1)\n",
    "\n",
    "# The rest of the data will be the training data.\n",
    "train_size = len(torch_dataset) - val_size\n",
    "\n",
    "# Split the training data into training and validation sets.\n",
    "train_ds, val_ds = random_split(torch_dataset, [train_size,val_size])\n",
    "\n",
    "# Create data loaders for the training and validation sets.\n",
    "# This will allow us to load data in batches.\n",
    "train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, BATCH_SIZE*2, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\" : train_dl, \"val\":val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\"train\" : len(train_ds), \"val\":len(val_ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 338, 'val': 37}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001, betas=(0.9, 0.999))\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists for graph generation\n",
    "epoch_counter_train = []\n",
    "epoch_counter_val = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x362dbd8d0>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x362dcd750>}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch +1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #For graph generation\n",
    "            if phase == \"train\":\n",
    "                train_loss.append(running_loss/dataset_sizes[phase])\n",
    "                train_acc.append(running_corrects / dataset_sizes[phase])\n",
    "                epoch_counter_train.append(epoch)\n",
    "            if phase == \"val\":\n",
    "                val_loss.append(running_loss/ dataset_sizes[phase])\n",
    "                val_acc.append(running_corrects / dataset_sizes[phase])\n",
    "                epoch_counter_val.append(epoch)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            #for printing        \n",
    "            if phase == \"train\":    \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            if phase == \"val\":    \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            \n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "train Loss: 1.9078 Acc: 0.3136\n",
      "val Loss: 1.4083 Acc: 0.3784\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "train Loss: 1.4217 Acc: 0.3994\n",
      "val Loss: 1.6478 Acc: 0.3784\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "train Loss: 1.4439 Acc: 0.4408\n",
      "val Loss: 1.7679 Acc: 0.5676\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "train Loss: 1.4096 Acc: 0.4852\n",
      "val Loss: 29.3191 Acc: 0.2162\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "train Loss: 1.4181 Acc: 0.4172\n",
      "val Loss: 1.5019 Acc: 0.4865\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "train Loss: 1.2882 Acc: 0.4734\n",
      "val Loss: 1.4903 Acc: 0.3514\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "train Loss: 1.2372 Acc: 0.4645\n",
      "val Loss: 1.2320 Acc: 0.5135\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "train Loss: 1.0583 Acc: 0.5888\n",
      "val Loss: 1.1834 Acc: 0.5676\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "train Loss: 0.9927 Acc: 0.6036\n",
      "val Loss: 1.0217 Acc: 0.5405\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "train Loss: 0.9638 Acc: 0.6213\n",
      "val Loss: 0.9422 Acc: 0.5676\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "train Loss: 0.9259 Acc: 0.6805\n",
      "val Loss: 0.9830 Acc: 0.5135\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "train Loss: 0.8313 Acc: 0.6953\n",
      "val Loss: 0.8650 Acc: 0.6486\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "train Loss: 0.8841 Acc: 0.6302\n",
      "val Loss: 0.9300 Acc: 0.5676\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "train Loss: 0.8007 Acc: 0.7160\n",
      "val Loss: 0.8939 Acc: 0.5946\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "train Loss: 0.7968 Acc: 0.7012\n",
      "val Loss: 0.8709 Acc: 0.5946\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "train Loss: 0.8023 Acc: 0.6923\n",
      "val Loss: 0.8888 Acc: 0.5946\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "train Loss: 0.7272 Acc: 0.7308\n",
      "val Loss: 0.9168 Acc: 0.5676\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "train Loss: 0.7220 Acc: 0.7249\n",
      "val Loss: 0.9006 Acc: 0.5405\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "train Loss: 0.7724 Acc: 0.7219\n",
      "val Loss: 0.9307 Acc: 0.6486\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "train Loss: 0.6862 Acc: 0.7604\n",
      "val Loss: 0.9018 Acc: 0.5946\n",
      "\n",
      "Training complete in 3m 53s\n",
      "Best val Acc: 0.648649\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "letstrythis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
